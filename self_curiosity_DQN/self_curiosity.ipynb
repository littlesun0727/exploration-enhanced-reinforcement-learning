{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conge\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = 2* self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q = tf.constant([[1.,2.,3.],[2.,3.,1.]])\n",
    "# a_new = tf.argmax(Q, axis=0)\n",
    "# sess = tf.Session()\n",
    "# sess.run(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural net Q_\\theta(s,a) as a class\n",
    "\n",
    "class Qfunction(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, sess, optimizer,lr=0.5, th = 1):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        sess: sess to execute this Qfunction\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # build the prediction graph\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        \n",
    "        #Construct a FNN with 1 hidden layer\n",
    "        Layer1_nodes = 24\n",
    "        W1 = tf.get_variable('W1',[obssize, Layer1_nodes],initializer = tf.truncated_normal_initializer(stddev=.1))\n",
    "        b1 = tf.get_variable('b1',[Layer1_nodes],initializer = tf.truncated_normal_initializer(stddev=.1))\n",
    "        h1 = tf.nn.relu(tf.matmul(state,W1) + b1)\n",
    "        \n",
    "#         Layer2_nodes = 24\n",
    "#         W2 = tf.get_variable('W2',[Layer1_nodes, Layer2_nodes],initializer = tf.truncated_normal_initializer(stddev=.1))\n",
    "#         b2 = tf.get_variable('b2',[Layer2_nodes],initializer = tf.truncated_normal_initializer(stddev=.1))\n",
    "#         h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "        \n",
    "        W2 = tf.get_variable('W2',[Layer1_nodes, actsize],initializer = tf.truncated_normal_initializer(stddev=.1))\n",
    "        b2 = tf.get_variable('b2',[actsize],initializer = tf.truncated_normal_initializer(stddev=.1))\n",
    "        h2 = tf.matmul(h1, W2) + b2        \n",
    "        \n",
    "        Qvalues = h2  # make sure it has size [None, actsize]\n",
    "        \n",
    "        # build the targets and actions\n",
    "        # targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "        # actions represent a_t\n",
    "        targets = tf.placeholder(tf.float32, [None])\n",
    "        actions = tf.placeholder(tf.int32, [None])\n",
    "        actions_one_hot = tf.one_hot(actions, actsize)\n",
    "        Qpreds = tf.reduce_sum(tf.multiply(h2, actions_one_hot), axis=1) # make sure it has size [None]\n",
    "        loss_function_1 = tf.reduce_mean(tf.square(Qpreds - targets))\n",
    "        \n",
    "        \n",
    "#         a_new = tf.argmax(Qvalues, axis=1)\n",
    "#         a_new_one_hot = tf.one_hot(a_new, actsize)\n",
    "#         a_diff = a_new_one_hot - actions_one_hot\n",
    "#         loss_function_2 = tf.norm(a_diff, ord = 'fro', axis = (0,1))\n",
    "        \n",
    "        \n",
    "#         Q_olds = tf.placeholder(tf.float32, [None, actsize])\n",
    "#         dist_new = tf.distributions.Categorical(probs = tf.nn.softmax(Qvalues))\n",
    "#         dist_old = tf.distributions.Categorical(probs = tf.nn.softmax(Q_olds))\n",
    "#         loss_function_2 = tf.reduce_sum(tf.distributions.kl_divergence(dist_new, dist_old))\n",
    "        \n",
    "#         self.lr = lr\n",
    "#         self.th = th\n",
    "        \n",
    "#         l1 = tf.cast(tf.less(loss_function_2, self.th), tf.float32)\n",
    "#         l2 = tf.cast(tf.greater(loss_function_2, self.th), tf.float32)\n",
    "#         self.lr = self.lr *(1.01*l1+0.99*l2)\n",
    "#         self.lr = 0.999\n",
    "        \n",
    "        loss = loss_function_1\n",
    "        \n",
    "        # optimization\n",
    "        self.train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.Qvalues = Qvalues\n",
    "        self.state = state\n",
    "        self.actions = actions\n",
    "        self.targets = targets\n",
    "        self.loss = loss\n",
    "        self.sess = sess\n",
    "#         self.a_new = a_new\n",
    "#         self.Q_olds = Q_olds\n",
    "        \n",
    "        \n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.Qvalues, feed_dict={self.state: states})\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        return self.sess.run([self.loss,self.train_op], feed_dict={self.state:states, self.actions:actions, self.targets:targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        if batchsize < self.number:\n",
    "            minibatch = random.sample(self.buffer, batchsize) \n",
    "        else:\n",
    "            minibatch = random.sample(self.buffer, self.number) \n",
    "        return minibatch  # need implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target_update(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_scope)\n",
    "    op = []\n",
    "    for v1, v2 in zip(from_vars, to_vars):\n",
    "        op.append(v2.assign(0.2*v1+0.8*v2))\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class state_matrix(object):\n",
    "#     def __init__(self, obssize):\n",
    "#         \"\"\"\n",
    "#         obssize: size of state\n",
    "#         \"\"\"\n",
    "#         self.obssize = int(np.sqrt(obssize))\n",
    "#         self.matrix = np.zeros((self.obssize, self.obssize))\n",
    "#         self.matrix[0][0] = 1\n",
    "    \n",
    "#     def reward(self, state):\n",
    "#         \"\"\"\n",
    "#         state: the state that is just visited\n",
    "#         \"\"\"\n",
    "#         rat_row, rat_col, _ = state\n",
    "#         if self.matrix[rat_row, rat_col] == 0:\n",
    "#             self.matrix[rat_row, rat_col] = 1\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "# maze = np.array([\n",
    "#      [ 1.,  0.,  1.,  1.,  1.,  0.,  1.],\n",
    "#      [ 1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "#      [ 1.,  1.,  1.,  0.,  1.,  1.,  1.],\n",
    "#      [ 0.,  1.,  1.,  1.,  1.,  1.,  0.],\n",
    "#      [ 1.,  1.,  0.,  0.,  1.,  1.,  1.],\n",
    "#      [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#      [ 1.,  1.,  1.,  1.,  0.,  1.,  1.]\n",
    "# ])\n",
    "\n",
    "# maze = np.array([\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]\n",
    "# ])\n",
    "\n",
    "\n",
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "rat_cell = (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze, file_name = 'Maze'):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    plt.savefig(file_name)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Qmaze(maze)\n",
    "# show(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obs(obs, maze_size = 7):\n",
    "    obs = obs.reshape((maze_size, maze_size))\n",
    "    for i in range(maze_size):\n",
    "        for j in range(maze_size):\n",
    "            if obs[i,j] == .5:\n",
    "                return (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (2,2)\n",
    "np.zeros((7,7))[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 episode.reward=-25.37.\n",
      "Evaluation reward=0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conge\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2 episode.reward=-22.27.\n",
      "Finished 3 episode.reward=-23.819999999999997.\n",
      "Finished 4 episode.reward=-22.77.\n",
      "Finished 5 episode.reward=-24.45.\n",
      "Finished 6 episode.reward=-23.9.\n",
      "Evaluation reward=0.\n",
      "Finished 7 episode.reward=-20.879999999999992.\n",
      "Finished 8 episode.reward=-22.139999999999993.\n",
      "Finished 9 episode.reward=-23.4.\n",
      "Finished 10 episode.reward=-21.929999999999993.\n",
      "Finished 11 episode.reward=-24.769999999999992.\n",
      "Evaluation reward=0.\n",
      "Finished 12 episode.reward=-22.689999999999998.\n",
      "Finished 13 episode.reward=-22.139999999999993.\n",
      "Finished 14 episode.reward=-24.45.\n",
      "Finished 15 episode.reward=-24.399999999999995.\n",
      "Finished 16 episode.reward=-23.979999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 17 episode.reward=-20.589999999999986.\n",
      "Finished 18 episode.reward=-25.11.\n",
      "Finished 19 episode.reward=-26.819999999999997.\n",
      "Finished 20 episode.reward=-25.479999999999993.\n",
      "Finished 21 episode.reward=-24.979999999999993.\n",
      "Evaluation reward=0.\n",
      "Finished 22 episode.reward=-23.189999999999998.\n",
      "Finished 23 episode.reward=-26.55999999999999.\n",
      "Finished 24 episode.reward=-24.769999999999996.\n",
      "Finished 25 episode.reward=-25.82.\n",
      "Finished 26 episode.reward=-22.42999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 27 episode.reward=-23.979999999999993.\n",
      "Finished 28 episode.reward=-25.32.\n",
      "Finished 29 episode.reward=-26.609999999999992.\n",
      "Finished 30 episode.reward=-24.769999999999996.\n",
      "Finished 31 episode.reward=-24.689999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 32 episode.reward=-24.719999999999995.\n",
      "Finished 33 episode.reward=-23.719999999999995.\n",
      "Finished 34 episode.reward=-25.219999999999988.\n",
      "Finished 35 episode.reward=-15.619999999999987.\n",
      "Finished 36 episode.reward=-25.740000000000002.\n",
      "Evaluation reward=0.\n",
      "Finished 37 episode.reward=-24.139999999999993.\n",
      "Finished 38 episode.reward=-30.269999999999992.\n",
      "Finished 39 episode.reward=-27.740000000000002.\n",
      "Finished 40 episode.reward=-27.899999999999995.\n",
      "Finished 41 episode.reward=-26.479999999999993.\n",
      "Evaluation reward=0.\n",
      "Finished 42 episode.reward=-27.05999999999999.\n",
      "Finished 43 episode.reward=-29.819999999999997.\n",
      "Finished 44 episode.reward=-30.399999999999995.\n",
      "Finished 45 episode.reward=-26.689999999999998.\n",
      "Finished 46 episode.reward=-28.269999999999996.\n",
      "Evaluation reward=0.\n",
      "Finished 47 episode.reward=-26.61.\n",
      "Finished 48 episode.reward=-26.61.\n",
      "Finished 49 episode.reward=-31.109999999999996.\n",
      "Finished 50 episode.reward=-25.139999999999997.\n",
      "Finished 51 episode.reward=-26.639999999999993.\n",
      "Evaluation reward=0.\n",
      "Finished 52 episode.reward=-25.21999999999999.\n",
      "Finished 53 episode.reward=-26.009999999999998.\n",
      "Finished 54 episode.reward=-24.63999999999999.\n",
      "Finished 55 episode.reward=-29.319999999999997.\n",
      "Finished 56 episode.reward=-26.769999999999992.\n",
      "Evaluation reward=0.\n",
      "Finished 57 episode.reward=-26.139999999999997.\n",
      "Finished 58 episode.reward=-30.559999999999995.\n",
      "Finished 59 episode.reward=-20.32999999999999.\n",
      "Finished 60 episode.reward=-25.01.\n",
      "Finished 61 episode.reward=-28.479999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 62 episode.reward=-24.139999999999993.\n",
      "Finished 63 episode.reward=-30.11.\n",
      "Finished 64 episode.reward=-26.479999999999997.\n",
      "Finished 65 episode.reward=-25.639999999999997.\n",
      "Finished 66 episode.reward=-22.69.\n",
      "Evaluation reward=0.\n",
      "Finished 67 episode.reward=-28.26999999999999.\n",
      "Finished 68 episode.reward=-24.82.\n",
      "Finished 69 episode.reward=-18.98999999999998.\n",
      "Finished 70 episode.reward=-25.639999999999993.\n",
      "Finished 71 episode.reward=-28.479999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 72 episode.reward=-26.269999999999992.\n",
      "Finished 73 episode.reward=-25.479999999999997.\n",
      "Finished 74 episode.reward=-28.479999999999997.\n",
      "Finished 75 episode.reward=-28.189999999999998.\n",
      "Finished 76 episode.reward=-26.13999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 77 episode.reward=-31.559999999999995.\n",
      "Finished 78 episode.reward=-13.329999999999984.\n",
      "Finished 79 episode.reward=-28.189999999999998.\n",
      "Finished 80 episode.reward=-29.89999999999999.\n",
      "Finished 81 episode.reward=-28.399999999999995.\n",
      "Evaluation reward=0.\n",
      "Finished 82 episode.reward=-32.739999999999995.\n",
      "Finished 83 episode.reward=-28.139999999999986.\n",
      "Finished 84 episode.reward=-28.819999999999997.\n",
      "Finished 85 episode.reward=-32.71999999999999.\n",
      "Finished 86 episode.reward=-29.189999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 87 episode.reward=-29.82.\n",
      "Finished 88 episode.reward=-30.56.\n",
      "Finished 89 episode.reward=-30.979999999999997.\n",
      "Finished 90 episode.reward=-35.42999999999999.\n",
      "Finished 91 episode.reward=-25.619999999999983.\n",
      "Evaluation reward=0.\n",
      "Finished 92 episode.reward=-25.61.\n",
      "Finished 93 episode.reward=-27.089999999999993.\n",
      "Finished 94 episode.reward=-29.219999999999988.\n",
      "Finished 95 episode.reward=-27.50999999999999.\n",
      "Finished 96 episode.reward=-25.27.\n",
      "Evaluation reward=0.\n",
      "Finished 97 episode.reward=-34.769999999999996.\n",
      "Finished 98 episode.reward=-32.13999999999999.\n",
      "Finished 99 episode.reward=-28.61.\n",
      "Finished 100 episode.reward=-27.4.\n",
      "Finished 101 episode.reward=-28.14.\n",
      "Evaluation reward=0.\n",
      "Finished 102 episode.reward=-34.349999999999994.\n",
      "Finished 103 episode.reward=-25.429999999999993.\n",
      "Finished 104 episode.reward=-28.879999999999992.\n",
      "Finished 105 episode.reward=-27.799999999999986.\n",
      "Finished 106 episode.reward=-30.71999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 107 episode.reward=-32.349999999999994.\n",
      "Finished 108 episode.reward=-24.45999999999999.\n",
      "Finished 109 episode.reward=-26.559999999999995.\n",
      "Finished 110 episode.reward=-11.619999999999994.\n",
      "Finished 111 episode.reward=-33.89999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 112 episode.reward=-36.139999999999986.\n",
      "Finished 113 episode.reward=-27.139999999999993.\n",
      "Finished 114 episode.reward=-25.03999999999999.\n",
      "Finished 115 episode.reward=-25.139999999999993.\n",
      "Finished 116 episode.reward=-26.769999999999996.\n",
      "Evaluation reward=0.\n",
      "Finished 117 episode.reward=-33.19.\n",
      "Finished 118 episode.reward=-26.71999999999999.\n",
      "Finished 119 episode.reward=-29.479999999999993.\n",
      "Finished 120 episode.reward=-32.19.\n",
      "Finished 121 episode.reward=-30.009999999999994.\n",
      "Evaluation reward=0.\n",
      "Finished 122 episode.reward=-29.139999999999997.\n",
      "Finished 123 episode.reward=-23.639999999999993.\n",
      "Finished 124 episode.reward=-30.349999999999998.\n",
      "Finished 125 episode.reward=-33.769999999999996.\n",
      "Finished 126 episode.reward=-23.61.\n",
      "Evaluation reward=0.\n",
      "Finished 127 episode.reward=-28.769999999999996.\n",
      "Finished 128 episode.reward=-23.11.\n",
      "Finished 129 episode.reward=-37.509999999999984.\n",
      "Finished 130 episode.reward=-30.039999999999985.\n",
      "Finished 131 episode.reward=-24.35.\n",
      "Evaluation reward=0.\n",
      "Finished 132 episode.reward=-28.009999999999987.\n",
      "Finished 133 episode.reward=-25.79999999999999.\n",
      "Finished 134 episode.reward=-31.219999999999995.\n",
      "Finished 135 episode.reward=-33.19.\n",
      "Finished 136 episode.reward=-24.34999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 137 episode.reward=-33.82.\n",
      "Finished 138 episode.reward=-25.4.\n",
      "Finished 139 episode.reward=-16.03999999999999.\n",
      "Finished 140 episode.reward=-26.13999999999999.\n",
      "Finished 141 episode.reward=-19.03999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 142 episode.reward=-29.85.\n",
      "Finished 143 episode.reward=-31.139999999999997.\n",
      "Finished 144 episode.reward=-28.189999999999998.\n",
      "Finished 145 episode.reward=-30.979999999999997.\n",
      "Finished 146 episode.reward=-27.40999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 147 episode.reward=-35.72.\n",
      "Finished 148 episode.reward=-28.43.\n",
      "Finished 149 episode.reward=-26.50999999999999.\n",
      "Finished 150 episode.reward=-28.639999999999997.\n",
      "Finished 151 episode.reward=-31.95.\n",
      "Evaluation reward=0.\n",
      "Finished 152 episode.reward=-27.749999999999986.\n",
      "Finished 153 episode.reward=-31.689999999999998.\n",
      "Finished 154 episode.reward=-24.979999999999997.\n",
      "Finished 155 episode.reward=-34.56.\n",
      "Finished 156 episode.reward=-26.40999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 157 episode.reward=-25.479999999999997.\n",
      "Finished 158 episode.reward=-30.059999999999995.\n",
      "Finished 159 episode.reward=-39.58999999999999.\n",
      "Finished 160 episode.reward=-38.539999999999985.\n",
      "Finished 161 episode.reward=-27.929999999999993.\n",
      "Evaluation reward=0.\n",
      "Finished 162 episode.reward=-30.139999999999993.\n",
      "Finished 163 episode.reward=-30.669999999999987.\n",
      "Finished 164 episode.reward=-26.48.\n",
      "Finished 165 episode.reward=-28.929999999999996.\n",
      "Finished 166 episode.reward=-27.169999999999987.\n",
      "Evaluation reward=0.\n",
      "Finished 167 episode.reward=-30.539999999999985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 168 episode.reward=-25.429999999999996.\n",
      "Finished 169 episode.reward=-17.079999999999988.\n",
      "Finished 170 episode.reward=-34.37999999999999.\n",
      "Finished 171 episode.reward=-8.330000000000002.\n",
      "Evaluation reward=0.\n",
      "Finished 172 episode.reward=-27.769999999999996.\n",
      "Finished 173 episode.reward=-32.22.\n",
      "Finished 174 episode.reward=-29.429999999999993.\n",
      "Finished 175 episode.reward=-27.899999999999995.\n",
      "Finished 176 episode.reward=-28.479999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 177 episode.reward=-29.36999999999999.\n",
      "Finished 178 episode.reward=-25.9.\n",
      "Finished 179 episode.reward=-23.769999999999996.\n",
      "Finished 180 episode.reward=-30.78999999999999.\n",
      "Finished 181 episode.reward=-26.82.\n",
      "Evaluation reward=0.\n",
      "Finished 182 episode.reward=-26.479999999999997.\n",
      "Finished 183 episode.reward=-24.4.\n",
      "Finished 184 episode.reward=-26.139999999999993.\n",
      "Finished 185 episode.reward=-29.14.\n",
      "Finished 186 episode.reward=-29.219999999999995.\n",
      "Evaluation reward=0.\n",
      "Finished 187 episode.reward=-21.07999999999999.\n",
      "Finished 188 episode.reward=-30.55999999999999.\n",
      "Finished 189 episode.reward=-30.42999999999999.\n",
      "Finished 190 episode.reward=-26.689999999999998.\n",
      "Finished 191 episode.reward=-26.409999999999986.\n",
      "Evaluation reward=0.\n",
      "Finished 192 episode.reward=-23.689999999999998.\n",
      "Finished 193 episode.reward=-25.61.\n",
      "Finished 194 episode.reward=-25.609999999999992.\n",
      "Finished 195 episode.reward=-24.9.\n",
      "Finished 196 episode.reward=-27.109999999999996.\n",
      "Evaluation reward=0.\n",
      "Finished 197 episode.reward=-36.79999999999999.\n",
      "Finished 198 episode.reward=-33.60999999999999.\n",
      "Finished 199 episode.reward=-24.979999999999997.\n",
      "Finished 200 episode.reward=-39.11.\n",
      "Finished 201 episode.reward=-28.61.\n",
      "Evaluation reward=0.\n",
      "Finished 202 episode.reward=-22.139999999999993.\n",
      "Finished 203 episode.reward=-29.979999999999997.\n",
      "Finished 204 episode.reward=-30.45999999999999.\n",
      "Finished 205 episode.reward=-29.849999999999994.\n",
      "Finished 206 episode.reward=-32.059999999999995.\n",
      "Evaluation reward=0.\n",
      "Finished 207 episode.reward=-29.639999999999993.\n",
      "Finished 208 episode.reward=-27.14.\n",
      "Finished 209 episode.reward=-31.169999999999987.\n",
      "Finished 210 episode.reward=-26.929999999999993.\n",
      "Finished 211 episode.reward=-25.829999999999988.\n",
      "Evaluation reward=0.\n",
      "Finished 212 episode.reward=-27.719999999999995.\n",
      "Finished 213 episode.reward=-23.299999999999994.\n",
      "Finished 214 episode.reward=-27.08999999999999.\n",
      "Finished 215 episode.reward=-30.03999999999999.\n",
      "Finished 216 episode.reward=-27.349999999999994.\n",
      "Evaluation reward=0.\n",
      "Finished 217 episode.reward=-27.14.\n",
      "Finished 218 episode.reward=-28.43.\n",
      "Finished 219 episode.reward=-23.64.\n",
      "Finished 220 episode.reward=-26.06.\n",
      "Finished 221 episode.reward=-32.29999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 222 episode.reward=-29.979999999999997.\n",
      "Finished 223 episode.reward=-16.28999999999999.\n",
      "Finished 224 episode.reward=-23.93.\n",
      "Finished 225 episode.reward=-33.19.\n",
      "Finished 226 episode.reward=-29.349999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 227 episode.reward=-31.87999999999999.\n",
      "Finished 228 episode.reward=-20.589999999999993.\n",
      "Finished 229 episode.reward=-27.089999999999986.\n",
      "Finished 230 episode.reward=-22.689999999999998.\n",
      "Finished 231 episode.reward=-32.829999999999984.\n",
      "Evaluation reward=0.\n",
      "Finished 232 episode.reward=-23.039999999999985.\n",
      "Finished 233 episode.reward=-23.139999999999993.\n",
      "Finished 234 episode.reward=-33.639999999999986.\n",
      "Finished 235 episode.reward=-25.11.\n",
      "Finished 236 episode.reward=-28.559999999999995.\n",
      "Evaluation reward=0.\n",
      "Finished 237 episode.reward=-30.32999999999999.\n",
      "Finished 238 episode.reward=-32.8.\n",
      "Finished 239 episode.reward=-24.269999999999996.\n",
      "Finished 240 episode.reward=-21.849999999999998.\n",
      "Finished 241 episode.reward=-23.11.\n",
      "Evaluation reward=0.\n",
      "Finished 242 episode.reward=-28.71999999999999.\n",
      "Finished 243 episode.reward=-24.189999999999998.\n",
      "Finished 244 episode.reward=-22.71999999999999.\n",
      "Finished 245 episode.reward=-33.58999999999999.\n",
      "Finished 246 episode.reward=-33.059999999999995.\n",
      "Evaluation reward=0.\n",
      "Finished 247 episode.reward=-25.639999999999993.\n",
      "Finished 248 episode.reward=-23.319999999999997.\n",
      "Finished 249 episode.reward=-25.169999999999995.\n",
      "Finished 250 episode.reward=-36.12.\n",
      "Finished 251 episode.reward=-23.58999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 252 episode.reward=-25.61.\n",
      "Finished 253 episode.reward=-22.71999999999999.\n",
      "Finished 254 episode.reward=-21.82999999999999.\n",
      "Finished 255 episode.reward=-20.78999999999999.\n",
      "Finished 256 episode.reward=-12.579999999999988.\n",
      "Evaluation reward=0.\n",
      "Finished 257 episode.reward=-32.059999999999995.\n",
      "Finished 258 episode.reward=-25.059999999999995.\n",
      "Finished 259 episode.reward=-26.11.\n",
      "Finished 260 episode.reward=-33.010000000000005.\n",
      "Finished 261 episode.reward=-23.71999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 262 episode.reward=-22.139999999999993.\n",
      "Finished 263 episode.reward=-13.789999999999996.\n",
      "Finished 264 episode.reward=-25.61.\n",
      "Finished 265 episode.reward=-19.409999999999986.\n",
      "Finished 266 episode.reward=-22.479999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 267 episode.reward=-27.639999999999997.\n",
      "Finished 268 episode.reward=-35.82999999999999.\n",
      "Finished 269 episode.reward=-24.479999999999997.\n",
      "Finished 270 episode.reward=-23.139999999999993.\n",
      "Finished 271 episode.reward=-26.19.\n",
      "Evaluation reward=0.\n",
      "Finished 272 episode.reward=-31.48.\n",
      "Finished 273 episode.reward=-28.319999999999997.\n",
      "Finished 274 episode.reward=-25.559999999999995.\n",
      "Finished 275 episode.reward=-35.71999999999999.\n",
      "Finished 276 episode.reward=-27.11.\n",
      "Evaluation reward=0.\n",
      "Finished 277 episode.reward=-24.689999999999998.\n",
      "Finished 278 episode.reward=-23.08999999999999.\n",
      "Finished 279 episode.reward=-27.00999999999999.\n",
      "Finished 280 episode.reward=-19.789999999999992.\n",
      "Finished 281 episode.reward=-19.539999999999992.\n",
      "Evaluation reward=0.\n",
      "Finished 282 episode.reward=-20.79999999999999.\n",
      "Finished 283 episode.reward=-24.639999999999993.\n",
      "Finished 284 episode.reward=-23.079999999999984.\n",
      "Finished 285 episode.reward=-25.98.\n",
      "Finished 286 episode.reward=-24.11.\n",
      "Evaluation reward=0.\n",
      "Finished 287 episode.reward=-21.63999999999999.\n",
      "Finished 288 episode.reward=-28.79999999999999.\n",
      "Finished 289 episode.reward=-32.81999999999999.\n",
      "Finished 290 episode.reward=-23.11.\n",
      "Finished 291 episode.reward=-31.319999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 292 episode.reward=-23.319999999999997.\n",
      "Finished 293 episode.reward=-23.11.\n",
      "Finished 294 episode.reward=-19.829999999999995.\n",
      "Finished 295 episode.reward=-23.979999999999993.\n",
      "Finished 296 episode.reward=-24.319999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 297 episode.reward=-20.37999999999999.\n",
      "Finished 298 episode.reward=-21.799999999999994.\n",
      "Finished 299 episode.reward=-16.78999999999999.\n",
      "Finished 300 episode.reward=-23.589999999999996.\n",
      "Finished 301 episode.reward=-23.32.\n",
      "Evaluation reward=0.\n",
      "Finished 302 episode.reward=-23.32.\n",
      "Finished 303 episode.reward=-30.59.\n",
      "Finished 304 episode.reward=-22.479999999999997.\n",
      "Finished 305 episode.reward=-22.689999999999998.\n",
      "Finished 306 episode.reward=-24.319999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 307 episode.reward=-30.29999999999999.\n",
      "Finished 308 episode.reward=-35.59.\n",
      "Finished 309 episode.reward=-35.42999999999999.\n",
      "Finished 310 episode.reward=-22.929999999999993.\n",
      "Finished 311 episode.reward=-22.71999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 312 episode.reward=-25.639999999999997.\n",
      "Finished 313 episode.reward=-22.349999999999994.\n",
      "Finished 314 episode.reward=-20.589999999999993.\n",
      "Finished 315 episode.reward=-23.61.\n",
      "Finished 316 episode.reward=-22.639999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 317 episode.reward=-22.639999999999997.\n",
      "Finished 318 episode.reward=-27.21999999999999.\n",
      "Finished 319 episode.reward=-22.689999999999998.\n",
      "Finished 320 episode.reward=-22.58999999999999.\n",
      "Finished 321 episode.reward=-28.03999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 322 episode.reward=-29.42999999999999.\n",
      "Finished 323 episode.reward=-31.299999999999986.\n",
      "Finished 324 episode.reward=-24.609999999999996.\n",
      "Finished 325 episode.reward=-25.21999999999999.\n",
      "Finished 326 episode.reward=-37.3.\n",
      "Evaluation reward=0.\n",
      "Finished 327 episode.reward=-22.479999999999997.\n",
      "Finished 328 episode.reward=-25.559999999999995.\n",
      "Finished 329 episode.reward=-25.32.\n",
      "Finished 330 episode.reward=-31.32.\n",
      "Finished 331 episode.reward=-23.53.\n",
      "Evaluation reward=0.\n",
      "Finished 332 episode.reward=-23.32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 333 episode.reward=-24.53.\n",
      "Finished 334 episode.reward=-27.29999999999999.\n",
      "Finished 335 episode.reward=-23.53.\n",
      "Finished 336 episode.reward=-36.53999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 337 episode.reward=-23.319999999999997.\n",
      "Finished 338 episode.reward=-23.319999999999997.\n",
      "Finished 339 episode.reward=-22.29999999999999.\n",
      "Finished 340 episode.reward=-21.639999999999993.\n",
      "Finished 341 episode.reward=-31.29999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 342 episode.reward=-31.349999999999994.\n",
      "Finished 343 episode.reward=-28.03.\n",
      "Finished 344 episode.reward=-23.53.\n",
      "Finished 345 episode.reward=-23.32.\n",
      "Finished 346 episode.reward=-23.11.\n",
      "Evaluation reward=0.\n",
      "Finished 347 episode.reward=-25.429999999999993.\n",
      "Finished 348 episode.reward=-23.32.\n",
      "Finished 349 episode.reward=-23.53.\n",
      "Finished 350 episode.reward=-29.29999999999999.\n",
      "Finished 351 episode.reward=-23.53.\n",
      "Evaluation reward=0.\n",
      "Finished 352 episode.reward=-23.11.\n",
      "Finished 353 episode.reward=-38.58999999999999.\n",
      "Finished 354 episode.reward=-20.58999999999999.\n",
      "Finished 355 episode.reward=-23.32.\n",
      "Finished 356 episode.reward=-22.689999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 357 episode.reward=-23.53.\n",
      "Finished 358 episode.reward=-23.53.\n",
      "Finished 359 episode.reward=-28.749999999999986.\n",
      "Finished 360 episode.reward=-24.429999999999993.\n",
      "Finished 361 episode.reward=-22.689999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 362 episode.reward=-22.689999999999998.\n",
      "Finished 363 episode.reward=-23.74.\n",
      "Finished 364 episode.reward=-23.53.\n",
      "Finished 365 episode.reward=-23.53.\n",
      "Finished 366 episode.reward=-23.529999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 367 episode.reward=-22.69.\n",
      "Finished 368 episode.reward=-23.32.\n",
      "Finished 369 episode.reward=-22.479999999999993.\n",
      "Finished 370 episode.reward=-22.479999999999997.\n",
      "Finished 371 episode.reward=-23.53.\n",
      "Evaluation reward=0.\n",
      "Finished 372 episode.reward=-23.529999999999998.\n",
      "Finished 373 episode.reward=-23.32.\n",
      "Finished 374 episode.reward=-23.53.\n",
      "Finished 375 episode.reward=-22.479999999999997.\n",
      "Finished 376 episode.reward=-36.879999999999995.\n",
      "Evaluation reward=0.\n",
      "Finished 377 episode.reward=-24.32.\n",
      "Finished 378 episode.reward=-23.53.\n",
      "Finished 379 episode.reward=-13.039999999999992.\n",
      "Finished 380 episode.reward=-23.32.\n",
      "Finished 381 episode.reward=-28.479999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 382 episode.reward=-23.32.\n",
      "Finished 383 episode.reward=-24.69.\n",
      "Finished 384 episode.reward=-23.11.\n",
      "Finished 385 episode.reward=-28.829999999999995.\n",
      "Finished 386 episode.reward=-22.21999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 387 episode.reward=-20.799999999999994.\n",
      "Finished 388 episode.reward=-25.929999999999993.\n",
      "Finished 389 episode.reward=-23.74.\n",
      "Finished 390 episode.reward=-26.3.\n",
      "Finished 391 episode.reward=-20.79999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 392 episode.reward=-20.37999999999999.\n",
      "Finished 393 episode.reward=-23.53.\n",
      "Finished 394 episode.reward=-21.00999999999999.\n",
      "Finished 395 episode.reward=-20.79999999999999.\n",
      "Finished 396 episode.reward=-22.689999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 397 episode.reward=-22.689999999999998.\n",
      "Finished 398 episode.reward=-30.53999999999999.\n",
      "Finished 399 episode.reward=-23.529999999999998.\n",
      "Finished 400 episode.reward=-36.81999999999999.\n",
      "Finished 401 episode.reward=-21.849999999999994.\n",
      "Evaluation reward=0.\n",
      "Finished 402 episode.reward=-22.689999999999998.\n",
      "Finished 403 episode.reward=-23.32.\n",
      "Finished 404 episode.reward=-24.09.\n",
      "Finished 405 episode.reward=-23.32.\n",
      "Finished 406 episode.reward=-21.00999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 407 episode.reward=-20.589999999999996.\n",
      "Finished 408 episode.reward=-23.53.\n",
      "Finished 409 episode.reward=-20.79999999999999.\n",
      "Finished 410 episode.reward=-23.319999999999997.\n",
      "Finished 411 episode.reward=-20.8.\n",
      "Evaluation reward=0.\n",
      "Finished 412 episode.reward=-34.00999999999999.\n",
      "Finished 413 episode.reward=-18.03999999999998.\n",
      "Finished 414 episode.reward=-23.74.\n",
      "Finished 415 episode.reward=-23.11.\n",
      "Finished 416 episode.reward=-23.32.\n",
      "Evaluation reward=0.\n",
      "Finished 417 episode.reward=-23.74.\n",
      "Finished 418 episode.reward=-23.95.\n",
      "Finished 419 episode.reward=-20.59.\n",
      "Finished 420 episode.reward=-23.319999999999997.\n",
      "Finished 421 episode.reward=-20.38.\n",
      "Evaluation reward=0.\n",
      "Finished 422 episode.reward=-23.319999999999997.\n",
      "Finished 423 episode.reward=-23.109999999999996.\n",
      "Finished 424 episode.reward=-24.59.\n",
      "Finished 425 episode.reward=-32.3.\n",
      "Finished 426 episode.reward=-23.319999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 427 episode.reward=-23.95.\n",
      "Finished 428 episode.reward=-18.039999999999992.\n",
      "Finished 429 episode.reward=-25.59.\n",
      "Finished 430 episode.reward=-25.829999999999995.\n",
      "Finished 431 episode.reward=-20.589999999999986.\n",
      "Evaluation reward=0.\n",
      "Finished 432 episode.reward=-21.58999999999999.\n",
      "Finished 433 episode.reward=-23.979999999999993.\n",
      "Finished 434 episode.reward=-28.08999999999999.\n",
      "Finished 435 episode.reward=-20.589999999999996.\n",
      "Finished 436 episode.reward=-23.95.\n",
      "Evaluation reward=0.\n",
      "Finished 437 episode.reward=-20.799999999999986.\n",
      "Finished 438 episode.reward=-23.32.\n",
      "Finished 439 episode.reward=-20.59.\n",
      "Finished 440 episode.reward=-23.95.\n",
      "Finished 441 episode.reward=-23.95.\n",
      "Evaluation reward=0.\n",
      "Finished 442 episode.reward=-11.829999999999998.\n",
      "Finished 443 episode.reward=-23.74.\n",
      "Finished 444 episode.reward=-20.589999999999986.\n",
      "Finished 445 episode.reward=-29.299999999999986.\n",
      "Finished 446 episode.reward=-23.74.\n",
      "Evaluation reward=0.\n",
      "Finished 447 episode.reward=-7.910000000000001.\n",
      "Finished 448 episode.reward=-24.09.\n",
      "Finished 449 episode.reward=-23.95.\n",
      "Finished 450 episode.reward=-27.589999999999986.\n",
      "Finished 451 episode.reward=-20.59.\n",
      "Evaluation reward=0.\n",
      "Finished 452 episode.reward=-20.249999999999996.\n",
      "Finished 453 episode.reward=-30.959999999999997.\n",
      "Finished 454 episode.reward=-25.589999999999986.\n",
      "Finished 455 episode.reward=-26.299999999999997.\n",
      "Finished 456 episode.reward=-15.539999999999985.\n",
      "Evaluation reward=0.\n",
      "Finished 457 episode.reward=-22.829999999999995.\n",
      "Finished 458 episode.reward=-20.959999999999983.\n",
      "Finished 459 episode.reward=-27.95.\n",
      "Finished 460 episode.reward=-20.379999999999985.\n",
      "Finished 461 episode.reward=-15.039999999999996.\n",
      "Evaluation reward=0.\n",
      "Finished 462 episode.reward=-20.58999999999999.\n",
      "Finished 463 episode.reward=-23.589999999999986.\n",
      "Finished 464 episode.reward=-23.319999999999997.\n",
      "Finished 465 episode.reward=-22.879999999999985.\n",
      "Finished 466 episode.reward=-6.04.\n",
      "Evaluation reward=0.\n",
      "Finished 467 episode.reward=-23.95.\n",
      "Finished 468 episode.reward=-20.589999999999986.\n",
      "Finished 469 episode.reward=-22.689999999999994.\n",
      "Finished 470 episode.reward=-25.37999999999999.\n",
      "Finished 471 episode.reward=-26.819999999999997.\n",
      "Evaluation reward=0.\n",
      "Finished 472 episode.reward=-23.319999999999997.\n",
      "Finished 473 episode.reward=-24.16.\n",
      "Finished 474 episode.reward=-7.410000000000001.\n",
      "Finished 475 episode.reward=-8.039999999999996.\n",
      "Finished 476 episode.reward=-12.039999999999996.\n",
      "Evaluation reward=0.\n",
      "Finished 477 episode.reward=-20.799999999999986.\n",
      "Finished 478 episode.reward=-34.59.\n",
      "Finished 479 episode.reward=-37.589999999999996.\n",
      "Finished 480 episode.reward=-20.589999999999993.\n",
      "Finished 481 episode.reward=-20.58999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 482 episode.reward=-24.37999999999999.\n",
      "Finished 483 episode.reward=-21.87999999999999.\n",
      "Finished 484 episode.reward=-19.329999999999995.\n",
      "Finished 485 episode.reward=-15.289999999999996.\n",
      "Finished 486 episode.reward=-20.37999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 487 episode.reward=-20.589999999999986.\n",
      "Finished 488 episode.reward=-23.32.\n",
      "Finished 489 episode.reward=-33.089999999999996.\n",
      "Finished 490 episode.reward=-24.799999999999997.\n",
      "Finished 491 episode.reward=-21.84999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 492 episode.reward=-20.58999999999999.\n",
      "Finished 493 episode.reward=-22.379999999999995.\n",
      "Finished 494 episode.reward=-22.269999999999992.\n",
      "Finished 495 episode.reward=-16.41.\n",
      "Finished 496 episode.reward=-13.539999999999992.\n",
      "Evaluation reward=0.\n",
      "Finished 497 episode.reward=-19.959999999999983.\n",
      "Finished 498 episode.reward=-21.84999999999999.\n",
      "Finished 499 episode.reward=-20.589999999999993.\n",
      "Finished 500 episode.reward=-20.589999999999986.\n",
      "Finished 501 episode.reward=-20.59.\n",
      "Evaluation reward=0.\n",
      "Finished 502 episode.reward=-20.58999999999999.\n",
      "Finished 503 episode.reward=-16.829999999999988.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 504 episode.reward=-21.429999999999993.\n",
      "Finished 505 episode.reward=-11.539999999999996.\n",
      "Finished 506 episode.reward=-21.849999999999994.\n",
      "Evaluation reward=0.\n",
      "Finished 507 episode.reward=-21.589999999999986.\n",
      "Finished 508 episode.reward=-27.589999999999986.\n",
      "Finished 509 episode.reward=-11.829999999999995.\n",
      "Finished 510 episode.reward=-20.59.\n",
      "Finished 511 episode.reward=-31.38.\n",
      "Evaluation reward=0.\n",
      "Finished 512 episode.reward=-29.039999999999992.\n",
      "Finished 513 episode.reward=-20.58999999999999.\n",
      "Finished 514 episode.reward=-22.269999999999996.\n",
      "Finished 515 episode.reward=-20.58999999999999.\n",
      "Finished 516 episode.reward=-34.90999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 517 episode.reward=-17.829999999999984.\n",
      "Finished 518 episode.reward=-10.039999999999992.\n",
      "Finished 519 episode.reward=-20.37999999999999.\n",
      "Finished 520 episode.reward=-16.829999999999984.\n",
      "Finished 521 episode.reward=-20.38.\n",
      "Evaluation reward=0.\n",
      "Finished 522 episode.reward=-23.95.\n",
      "Finished 523 episode.reward=-20.589999999999996.\n",
      "Finished 524 episode.reward=-20.58999999999999.\n",
      "Finished 525 episode.reward=-19.749999999999996.\n",
      "Finished 526 episode.reward=-7.04.\n",
      "Evaluation reward=0.\n",
      "Finished 527 episode.reward=-19.959999999999987.\n",
      "Finished 528 episode.reward=-4.540000000000001.\n",
      "Finished 529 episode.reward=-20.799999999999986.\n",
      "Finished 530 episode.reward=-23.32.\n",
      "Finished 531 episode.reward=-20.24999999999999.\n",
      "Evaluation reward=0.\n",
      "Finished 532 episode.reward=-22.479999999999997.\n",
      "Finished 533 episode.reward=-20.59.\n",
      "Finished 534 episode.reward=-20.799999999999994.\n",
      "Finished 535 episode.reward=-19.329999999999995.\n",
      "Finished 536 episode.reward=-23.299999999999994.\n",
      "Evaluation reward=0.\n",
      "Finished 537 episode.reward=-20.799999999999997.\n",
      "Finished 538 episode.reward=-20.37999999999999.\n",
      "Finished 539 episode.reward=-20.8.\n",
      "Finished 540 episode.reward=-20.8.\n",
      "Finished 541 episode.reward=-23.11.\n",
      "Evaluation reward=0.\n",
      "Finished 542 episode.reward=-20.799999999999986.\n",
      "Finished 543 episode.reward=-23.53.\n",
      "Finished 544 episode.reward=-20.59.\n",
      "Finished 545 episode.reward=-11.539999999999996.\n",
      "Finished 546 episode.reward=-23.529999999999998.\n",
      "Evaluation reward=0.\n",
      "Finished 547 episode.reward=-20.8.\n"
     ]
    }
   ],
   "source": [
    "# parameter initializations\n",
    "lr = 1e-3  # learning rate for gradient update\n",
    "batchsize = 100  # batchsize for buffer sampling\n",
    "maxlength = 10000  # max number of tuples held by buffer\n",
    "maxstep = 100\n",
    "\n",
    "tau = 100  # time steps for target update\n",
    "episodes = 1000 # number of episodes to run\n",
    "initialsize = 500  # initial time steps before start updating\n",
    "epsilon_min = 0.01 # constant for exploration\n",
    "epsilon_decay = 0.999\n",
    "gamma = .99  # discount\n",
    "\n",
    "reward_record = []\n",
    "evaluation_record = []\n",
    "\n",
    "# initialize environment\n",
    "env = Qmaze(maze)\n",
    "obssize = 49\n",
    "actsize = num_actions\n",
    "\n",
    "# initialize tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# initialize networks\n",
    "with tf.variable_scope(\"principal\"):\n",
    "    Qprincipal = Qfunction(obssize, actsize, sess, optimizer)\n",
    "with tf.variable_scope(\"target\"):\n",
    "    Qtarget = Qfunction(obssize, actsize, sess, optimizer)\n",
    "\n",
    "# build ops\n",
    "update = build_target_update(\"principal\", \"target\")  # call sess.run(update) to copy\n",
    "                                                     # from principal to target\n",
    "\n",
    "# initialization of graph and buffer\n",
    "sess.run(tf.global_variables_initializer())\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "sess.run(update)\n",
    "\n",
    "# main iteration\n",
    "# YOUR CODE HERE\n",
    "counter = 0\n",
    "epsilon =1.0\n",
    "for e in range(episodes):\n",
    "    state_mat = np.zeros((7,7))\n",
    "    \n",
    "    env.reset(rat_cell)\n",
    "    done = 'not_over'\n",
    "    rewardsum = 0\n",
    "    epsilon = max(0.1,epsilon * 0.995)\n",
    "    \n",
    "    obs = env.observe()\n",
    "    \n",
    "    for _ in range(maxstep):\n",
    "        valid_actions = env.valid_actions()\n",
    "        if not valid_actions: break\n",
    "        counter = counter + 1\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            values = Qprincipal.compute_Qvalues(obs)\n",
    "            action = np.argmax(values)\n",
    "\n",
    "        obs_, reward, done = env.act(action)\n",
    "        \n",
    "        # Store obs in state_matrix\n",
    "        pro_obs = process_obs(obs)\n",
    "        state_mat[pro_obs] = 1.0\n",
    "\n",
    "        # Modefied the reward by state curiosity\n",
    "        pro_obs_ = process_obs(obs_)\n",
    "        if state_mat[pro_obs_] == 0.0:\n",
    "            reward -= 0.04\n",
    "        elif pro_obs == pro_obs_:\n",
    "            reward -= 0.75\n",
    "        else:\n",
    "            reward -= 0.25\n",
    "            \n",
    "        rewardsum += reward\n",
    "#         print('rewardsum: ', rewardsum)\n",
    "        \n",
    "        #Implement buffer replay to store memory\n",
    "        experience = (obs, action, reward, obs_)\n",
    "        buffer.append(experience)\n",
    "        buffer.pop()\n",
    "        \n",
    "            \n",
    "        if counter>initialsize and counter%5 == 0:\n",
    "            #Sample from stored memory\n",
    "            Samples = buffer.sample(batchsize)\n",
    "\n",
    "            #Conpute target_i\n",
    "            states = []\n",
    "            actions = []\n",
    "            targets = []\n",
    "            for i in range(len(Samples)):\n",
    "                s_current, action, r, s_next = Samples[i]\n",
    "                Q_target = Qtarget.compute_Qvalues(s_next)\n",
    "                states.extend(s_current)\n",
    "                actions.append(action)\n",
    "                target = r + gamma * np.max(Q_target)\n",
    "                targets.append(target)\n",
    "\n",
    "            #Compute empirical loss, update theta\n",
    "            loss = Qprincipal.train(states, actions, targets)\n",
    "            \n",
    "        #Update target network\n",
    "        if counter % tau == 0:\n",
    "            sess.run(update)\n",
    "        if done == 'win':\n",
    "            break\n",
    "            \n",
    "        #Swap observation\n",
    "        obs = obs_\n",
    "    \n",
    "    reward_record.append(rewardsum)\n",
    "    \n",
    "    print('Finished {} episode.reward={}.'.format(e+1,rewardsum))\n",
    "    show(env, file_name = 'dive_maze/div_Maze'+'test_1_'+str(e+1))\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        state_mat = np.zeros((7,7))\n",
    "        \n",
    "        env.reset(rat_cell)\n",
    "        done = 'not_over'\n",
    "        rewardsum = 0\n",
    "        \n",
    "        obs = env.observe()\n",
    "        \n",
    "        for _ in range(50):\n",
    "            values = Qprincipal.compute_Qvalues(obs)\n",
    "            action = np.argmax(values)\n",
    "            obs_, reward, done = env.act(action)\n",
    "            \n",
    "            rewardsum += reward\n",
    "            \n",
    "            pro_obs = process_obs(obs)\n",
    "            state_mat[pro_obs] = 1.0\n",
    "            \n",
    "            pro_obs_ = process_obs(obs_)\n",
    "            if state_mat[pro_obs_] == 0.0:\n",
    "                reward -= 0.04\n",
    "            elif pro_obs == pro_obs_:\n",
    "                reward -= 0.75\n",
    "            else:\n",
    "                reward -= 0.25\n",
    "\n",
    "            \n",
    "            if done == 'win':\n",
    "                break\n",
    "            obs = obs_\n",
    "        evaluation_record.append(rewardsum)\n",
    "        print('Evaluation reward={}.'.format(rewardsum))\n",
    "        show(env, file_name = 'dive_maze_evaluation/div_Maze'+'test_1_'+str(e+1))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(200):\n",
    "    with open('Self_curiosity_rewardsEval_2.dat', 'a') as eval_reward_file:\n",
    "        if i < 10:\n",
    "            print(np.mean(evaluation_record[:i]), file=eval_reward_file)\n",
    "        else:\n",
    "            print(np.mean(evaluation_record[(i-10):i]), file=eval_reward_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    with open('Self_curiosity_rewards_2.dat', 'a') as eval_reward_file:\n",
    "        if i < 10:\n",
    "            print(np.mean(reward_record[:i+1]), file=eval_reward_file)\n",
    "        else:\n",
    "            print(np.mean(reward_record[(i-10):i]), file=eval_reward_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wzg: create a S*A matrix at the initialization step.  In each sample process, only to replace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to create an S matrix to store all the info about whether a state is visted or not. Use 0 or 1 to indicate the property. And reset the matrix each time when start a new epoch. But how to create S matrix?\n",
    "so recall the state is indicated by the coordiantes, so I should use a matrix, sqrtS * sqrtS dimension. All zero. each time we got the state output from action(), we change relavent element by adding 1. Let's just treat it as a blackbox function and then think about how to update. just use the name.\n",
    "after update S matrix, calculate a curiosity reward, and use a function to calculate it. then give the reward to this step.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
